{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArbazBamboowala/DeepLearning/blob/main/DeepLearning-Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa9FXRAJ58dn",
        "outputId": "45307f66-fe25-4572-ad93-200231df75a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[b'Hellow World' b'Hellow World']\n",
            " [b'Hellow World' b'Hellow World']\n",
            " [b'Hellow World' b'Hellow World']\n",
            " [b'Hellow World' b'Hellow World']\n",
            " [b'Hellow World' b'Hellow World']\n",
            " [b'Hellow World' b'Hellow World']], shape=(6, 2), dtype=string)\n",
            "tf.Tensor([4.6000004], shape=(1,), dtype=float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.framework.ops import enable_eager_execution\n",
        "\n",
        "tf.__version__\n",
        "#creating a tensor\n",
        "hello = tf.constant(\"Hello World\")\n",
        "\n",
        "\n",
        "hello_2 = tf.constant(\"Hellow World\", shape = [6,2], name=\"my_tensor\")\n",
        "print(hello_2)\n",
        "\n",
        "#What is Tensor?\n",
        "#Tensors are replacement for Numpy Array, or we can say it is an n dimensional data array\n",
        "#If numpy is same as tensor, then why use tensor?\n",
        "#numpy does not take advantage of GPU, same as Numpy array.\n",
        "#Numpy cannot use GPU\n",
        "#Add and do matrix multiplication we can use numpy but only using CPU, Tensor is Numpy array but powered by GPU\n",
        "\n",
        "a = tf.constant([1.2])\n",
        "b = tf.constant([3.4])\n",
        "c = tf.add(a,b)\n",
        "print(c)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import DEBUG\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework.ops import enable_eager_execution\n",
        "\n",
        "tf.executing_eagerly()\n",
        "#LOADING DATA\n",
        "(train_x, train_y),(_,_) = tf.keras.datasets.boston_housing.load_data(test_split=0)\n",
        "\n",
        "train_x.shape\n",
        "\n",
        "train_y.shape\n",
        "\n",
        "train_x.dtype #Here data is float64 i.e its in frame type and dont need gpu\n",
        "#so we convert datatype to float32 or we can use float16 as well\n",
        "\n",
        "train_x = train_x.astype('float32')\n",
        "\n",
        "train_y = train_y.astype('float32')\n",
        "\n",
        "train_x.shape\n",
        "train_y.shape\n",
        "\n",
        "#now I started building the model or a graph\n",
        "#lets initialize weights and bias with zero\n",
        "w = tf.zeros(shape=(13,1)) #these are tensors with value of 0, #shape is 13 because it has 13 features and 1 is because we do matrix multiplication, if no 1 then it will become a vector\n",
        "b = tf.zeros(shape=(1)) # b is value 1 because there are total 506 data and 13 features so we multiple 506 * 13 matrix with 506 * 1 so it will become 13,1\n",
        "\n",
        "\n",
        "#I just created a function for prediction\n",
        "def prediction(x, w, b, y_actual):\n",
        "  xw_matmul = tf.matmul(x, w)\n",
        "  y = tf.add(xw_matmul, b)\n",
        "  return y\n",
        "\n",
        "#Now calculating loss i.e actual and predicted value of y, then squaring the difference and then calculating the average.\n",
        "#reduce mean adds all 506 values.\n",
        "#MEAN SQUARED ERROR\n",
        "def loss(y_actual, y_predicted):\n",
        "  diff = y_actual - y_predicted\n",
        "  sqr = tf.square(diff)\n",
        "  avg = tf.reduce_mean(sqr)\n",
        "  return avg\n",
        "\n",
        "#Now we do gradient and then go for descent\n",
        "#GRADIENT FUNCTION\n",
        "def train(x,y_actual, w, b, learning_rate = 0.01):\n",
        "  #Lets record my mathematical operation on tape to calculate loss\n",
        "  with tf.GradientTape() as t:\n",
        "    t.watch([w,b])\n",
        "\n",
        "    current_prediction = prediction(x,w,b,train_y)\n",
        "    current_loss = loss(y_actual, current_prediction)\n",
        "\n",
        "    #Now I will calculate Gradient for Loss w.r.t to weights and bias\n",
        "\n",
        "  dw, db = t.gradient(current_loss,[w,b])\n",
        "\n",
        "#Now lets do the descent\n",
        "\n",
        "#Updating Weight and Bias\n",
        "  w = w - learning_rate*dw\n",
        "  b = b - learning_rate*db\n",
        "\n",
        "  return w, b\n",
        "\n",
        "#NOW TRAINING\n",
        "#Training for 100 times:\n",
        "for i in range(100):\n",
        "  w, b = train(train_x, train_y, w, b, learning_rate=0.01)\n",
        "  print('Current Loss on Iteration', i,\n",
        "        loss(train_y, prediction(train_x, w, b,train_y)).numpy())\n",
        "\n",
        "#Check Weights and Bias\n",
        "print('Weights: \\n', w.numpy())\n",
        "print('Bias: \\n', b.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9lKPLyzsH6U",
        "outputId": "3bde376f-997b-422c-cabe-fb56c71b6910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Loss on Iteration 0 19006894000.0\n",
            "Current Loss on Iteration 1 7.446026e+17\n",
            "Current Loss on Iteration 2 2.9173265e+25\n",
            "Current Loss on Iteration 3 1.1429992e+33\n",
            "Current Loss on Iteration 4 inf\n",
            "Current Loss on Iteration 5 inf\n",
            "Current Loss on Iteration 6 inf\n",
            "Current Loss on Iteration 7 inf\n",
            "Current Loss on Iteration 8 inf\n",
            "Current Loss on Iteration 9 inf\n",
            "Current Loss on Iteration 10 nan\n",
            "Current Loss on Iteration 11 nan\n",
            "Current Loss on Iteration 12 nan\n",
            "Current Loss on Iteration 13 nan\n",
            "Current Loss on Iteration 14 nan\n",
            "Current Loss on Iteration 15 nan\n",
            "Current Loss on Iteration 16 nan\n",
            "Current Loss on Iteration 17 nan\n",
            "Current Loss on Iteration 18 nan\n",
            "Current Loss on Iteration 19 nan\n",
            "Current Loss on Iteration 20 nan\n",
            "Current Loss on Iteration 21 nan\n",
            "Current Loss on Iteration 22 nan\n",
            "Current Loss on Iteration 23 nan\n",
            "Current Loss on Iteration 24 nan\n",
            "Current Loss on Iteration 25 nan\n",
            "Current Loss on Iteration 26 nan\n",
            "Current Loss on Iteration 27 nan\n",
            "Current Loss on Iteration 28 nan\n",
            "Current Loss on Iteration 29 nan\n",
            "Current Loss on Iteration 30 nan\n",
            "Current Loss on Iteration 31 nan\n",
            "Current Loss on Iteration 32 nan\n",
            "Current Loss on Iteration 33 nan\n",
            "Current Loss on Iteration 34 nan\n",
            "Current Loss on Iteration 35 nan\n",
            "Current Loss on Iteration 36 nan\n",
            "Current Loss on Iteration 37 nan\n",
            "Current Loss on Iteration 38 nan\n",
            "Current Loss on Iteration 39 nan\n",
            "Current Loss on Iteration 40 nan\n",
            "Current Loss on Iteration 41 nan\n",
            "Current Loss on Iteration 42 nan\n",
            "Current Loss on Iteration 43 nan\n",
            "Current Loss on Iteration 44 nan\n",
            "Current Loss on Iteration 45 nan\n",
            "Current Loss on Iteration 46 nan\n",
            "Current Loss on Iteration 47 nan\n",
            "Current Loss on Iteration 48 nan\n",
            "Current Loss on Iteration 49 nan\n",
            "Current Loss on Iteration 50 nan\n",
            "Current Loss on Iteration 51 nan\n",
            "Current Loss on Iteration 52 nan\n",
            "Current Loss on Iteration 53 nan\n",
            "Current Loss on Iteration 54 nan\n",
            "Current Loss on Iteration 55 nan\n",
            "Current Loss on Iteration 56 nan\n",
            "Current Loss on Iteration 57 nan\n",
            "Current Loss on Iteration 58 nan\n",
            "Current Loss on Iteration 59 nan\n",
            "Current Loss on Iteration 60 nan\n",
            "Current Loss on Iteration 61 nan\n",
            "Current Loss on Iteration 62 nan\n",
            "Current Loss on Iteration 63 nan\n",
            "Current Loss on Iteration 64 nan\n",
            "Current Loss on Iteration 65 nan\n",
            "Current Loss on Iteration 66 nan\n",
            "Current Loss on Iteration 67 nan\n",
            "Current Loss on Iteration 68 nan\n",
            "Current Loss on Iteration 69 nan\n",
            "Current Loss on Iteration 70 nan\n",
            "Current Loss on Iteration 71 nan\n",
            "Current Loss on Iteration 72 nan\n",
            "Current Loss on Iteration 73 nan\n",
            "Current Loss on Iteration 74 nan\n",
            "Current Loss on Iteration 75 nan\n",
            "Current Loss on Iteration 76 nan\n",
            "Current Loss on Iteration 77 nan\n",
            "Current Loss on Iteration 78 nan\n",
            "Current Loss on Iteration 79 nan\n",
            "Current Loss on Iteration 80 nan\n",
            "Current Loss on Iteration 81 nan\n",
            "Current Loss on Iteration 82 nan\n",
            "Current Loss on Iteration 83 nan\n",
            "Current Loss on Iteration 84 nan\n",
            "Current Loss on Iteration 85 nan\n",
            "Current Loss on Iteration 86 nan\n",
            "Current Loss on Iteration 87 nan\n",
            "Current Loss on Iteration 88 nan\n",
            "Current Loss on Iteration 89 nan\n",
            "Current Loss on Iteration 90 nan\n",
            "Current Loss on Iteration 91 nan\n",
            "Current Loss on Iteration 92 nan\n",
            "Current Loss on Iteration 93 nan\n",
            "Current Loss on Iteration 94 nan\n",
            "Current Loss on Iteration 95 nan\n",
            "Current Loss on Iteration 96 nan\n",
            "Current Loss on Iteration 97 nan\n",
            "Current Loss on Iteration 98 nan\n",
            "Current Loss on Iteration 99 nan\n",
            "Weights: \n",
            " [[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Bias: \n",
            " [nan]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WE SEE THAT THERE IS ALOT OF ERROR THAT IT BECAME NAN (NOT A NUMBER), so\n",
        "#NOW I will use normalizer instead\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework.ops import enable_eager_execution\n",
        "\n",
        "tf.executing_eagerly()\n",
        "from sklearn.preprocessing import Normalizer\n",
        "(train_x, train_y),(_,_) = tf.keras.datasets.boston_housing.load_data(test_split=0)\n",
        "\n",
        "train_x.shape\n",
        "\n",
        "train_y.shape\n",
        "\n",
        "train_x.dtype #Here data is float64 i.e its in frame type and dont need gpu\n",
        "#so we convert datatype to float32 or we can use float16 as well\n",
        "\n",
        "train_x = train_x.astype('float32')\n",
        "\n",
        "train_y = train_y.astype('float32')\n",
        "transformer = Normalizer()\n",
        "train_x = transformer.fit_transform(train_x)\n",
        "train_x[0]\n",
        "\n",
        "w = tf.zeros(shape=(13,1))\n",
        "b = tf.zeros(shape=(1))\n",
        "\n",
        "def prediction(x, w, b, y_actual):\n",
        "  xw_matmul = tf.matmul(x, w)\n",
        "  y = tf.add(xw_matmul, b)\n",
        "  return y\n",
        "def loss(y_actual, y_predicted):\n",
        "  diff = y_actual - y_predicted\n",
        "  sqr = tf.square(diff)\n",
        "  avg = tf.reduce_mean(sqr)\n",
        "  return avg\n",
        "\n",
        "#Now we do gradient and then go for descent\n",
        "#GRADIENT FUNCTION\n",
        "def train(x,y_actual, w, b, learning_rate = 0.01):\n",
        "  #Lets record my mathematical operation on tape to calculate loss\n",
        "  with tf.GradientTape() as t:\n",
        "    t.watch([w,b])\n",
        "\n",
        "    current_prediction = prediction(x,w,b,train_y)\n",
        "    current_loss = loss(y_actual, current_prediction)\n",
        "\n",
        "    #Now I will calculate Gradient for Loss w.r.t to weights and bias\n",
        "\n",
        "  dw, db = t.gradient(current_loss,[w,b])\n",
        "\n",
        "#Now lets do the descent\n",
        "\n",
        "#Updating Weight and Bias\n",
        "  w = w - learning_rate*dw\n",
        "  b = b - learning_rate*db\n",
        "\n",
        "  return w, b\n",
        "\n",
        "#NOW TRAINING\n",
        "#Training for 100 times:\n",
        "#We can train further to get little more accuracy, also check values of w and b\n",
        "for i in range(100):\n",
        "  w, b = train(train_x, train_y, w, b, learning_rate=0.01)\n",
        "  print('Current Loss on Iteration', i,\n",
        "        loss(train_y, prediction(train_x, w, b,train_y)).numpy())\n",
        "\n",
        "#Check Weights and Bias\n",
        "print('Weights: \\n', w.numpy())\n",
        "print('Bias: \\n', b.numpy())\n",
        "\n",
        "#NOW ERROR IS REDUCED FROM NAN\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-9QrgSLCgF3",
        "outputId": "e907de93-1a9f-43ea-b7ba-2080c22b09f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Loss on Iteration 0 553.7515\n",
            "Current Loss on Iteration 1 518.2617\n",
            "Current Loss on Iteration 2 485.45786\n",
            "Current Loss on Iteration 3 455.1365\n",
            "Current Loss on Iteration 4 427.1098\n",
            "Current Loss on Iteration 5 401.20413\n",
            "Current Loss on Iteration 6 377.25897\n",
            "Current Loss on Iteration 7 355.12595\n",
            "Current Loss on Iteration 8 334.66788\n",
            "Current Loss on Iteration 9 315.75803\n",
            "Current Loss on Iteration 10 298.2792\n",
            "Current Loss on Iteration 11 282.12317\n",
            "Current Loss on Iteration 12 267.1898\n",
            "Current Loss on Iteration 13 253.38655\n",
            "Current Loss on Iteration 14 240.62788\n",
            "Current Loss on Iteration 15 228.83473\n",
            "Current Loss on Iteration 16 217.93405\n",
            "Current Loss on Iteration 17 207.85834\n",
            "Current Loss on Iteration 18 198.54506\n",
            "Current Loss on Iteration 19 189.9366\n",
            "Current Loss on Iteration 20 181.9796\n",
            "Current Loss on Iteration 21 174.62473\n",
            "Current Loss on Iteration 22 167.82646\n",
            "Current Loss on Iteration 23 161.54263\n",
            "Current Loss on Iteration 24 155.73434\n",
            "Current Loss on Iteration 25 150.36557\n",
            "Current Loss on Iteration 26 145.40309\n",
            "Current Loss on Iteration 27 140.81613\n",
            "Current Loss on Iteration 28 136.57628\n",
            "Current Loss on Iteration 29 132.65729\n",
            "Current Loss on Iteration 30 129.03482\n",
            "Current Loss on Iteration 31 125.68647\n",
            "Current Loss on Iteration 32 122.5915\n",
            "Current Loss on Iteration 33 119.73073\n",
            "Current Loss on Iteration 34 117.086426\n",
            "Current Loss on Iteration 35 114.64221\n",
            "Current Loss on Iteration 36 112.38295\n",
            "Current Loss on Iteration 37 110.29462\n",
            "Current Loss on Iteration 38 108.36431\n",
            "Current Loss on Iteration 39 106.58006\n",
            "Current Loss on Iteration 40 104.93081\n",
            "Current Loss on Iteration 41 103.406334\n",
            "Current Loss on Iteration 42 101.99722\n",
            "Current Loss on Iteration 43 100.69471\n",
            "Current Loss on Iteration 44 99.49073\n",
            "Current Loss on Iteration 45 98.37785\n",
            "Current Loss on Iteration 46 97.34915\n",
            "Current Loss on Iteration 47 96.39827\n",
            "Current Loss on Iteration 48 95.51934\n",
            "Current Loss on Iteration 49 94.70689\n",
            "Current Loss on Iteration 50 93.955894\n",
            "Current Loss on Iteration 51 93.26171\n",
            "Current Loss on Iteration 52 92.620026\n",
            "Current Loss on Iteration 53 92.02687\n",
            "Current Loss on Iteration 54 91.47858\n",
            "Current Loss on Iteration 55 90.97176\n",
            "Current Loss on Iteration 56 90.50326\n",
            "Current Loss on Iteration 57 90.07018\n",
            "Current Loss on Iteration 58 89.66986\n",
            "Current Loss on Iteration 59 89.299805\n",
            "Current Loss on Iteration 60 88.957726\n",
            "Current Loss on Iteration 61 88.6415\n",
            "Current Loss on Iteration 62 88.34918\n",
            "Current Loss on Iteration 63 88.07896\n",
            "Current Loss on Iteration 64 87.82915\n",
            "Current Loss on Iteration 65 87.59823\n",
            "Current Loss on Iteration 66 87.38474\n",
            "Current Loss on Iteration 67 87.18739\n",
            "Current Loss on Iteration 68 87.00495\n",
            "Current Loss on Iteration 69 86.83628\n",
            "Current Loss on Iteration 70 86.68036\n",
            "Current Loss on Iteration 71 86.5362\n",
            "Current Loss on Iteration 72 86.40292\n",
            "Current Loss on Iteration 73 86.27971\n",
            "Current Loss on Iteration 74 86.16578\n",
            "Current Loss on Iteration 75 86.06046\n",
            "Current Loss on Iteration 76 85.96307\n",
            "Current Loss on Iteration 77 85.87304\n",
            "Current Loss on Iteration 78 85.78978\n",
            "Current Loss on Iteration 79 85.71281\n",
            "Current Loss on Iteration 80 85.641624\n",
            "Current Loss on Iteration 81 85.57579\n",
            "Current Loss on Iteration 82 85.51493\n",
            "Current Loss on Iteration 83 85.45864\n",
            "Current Loss on Iteration 84 85.406586\n",
            "Current Loss on Iteration 85 85.35844\n",
            "Current Loss on Iteration 86 85.31391\n",
            "Current Loss on Iteration 87 85.27273\n",
            "Current Loss on Iteration 88 85.234634\n",
            "Current Loss on Iteration 89 85.199394\n",
            "Current Loss on Iteration 90 85.16679\n",
            "Current Loss on Iteration 91 85.136635\n",
            "Current Loss on Iteration 92 85.108734\n",
            "Current Loss on Iteration 93 85.08292\n",
            "Current Loss on Iteration 94 85.05902\n",
            "Current Loss on Iteration 95 85.03692\n",
            "Current Loss on Iteration 96 85.01645\n",
            "Current Loss on Iteration 97 84.99751\n",
            "Current Loss on Iteration 98 84.97998\n",
            "Current Loss on Iteration 99 84.963745\n",
            "Weights: \n",
            " [[6.27857521e-02]\n",
            " [2.58572161e-01]\n",
            " [2.17821717e-01]\n",
            " [1.46146421e-03]\n",
            " [1.13587305e-02]\n",
            " [1.31812558e-01]\n",
            " [1.38818789e+00]\n",
            " [8.23873654e-02]\n",
            " [1.76484972e-01]\n",
            " [7.99328279e+00]\n",
            " [3.82081807e-01]\n",
            " [7.41107655e+00]\n",
            " [2.53885090e-01]]\n",
            "Bias: \n",
            " [11.476417]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "tf.__version__\n",
        "\n",
        "a = tf.constant([1.2], name = 'First', dtype = tf.float32)\n",
        "b = tf.constant([3.4], name = 'Second', dtype = tf.float32)\n",
        "c = tf.add(a,b, name = 'Final')\n",
        "print(c)\n",
        "\n",
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "print(sess.run(hello))\n"
      ],
      "metadata": {
        "id": "3wXhYR1F6fcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# USING KERAS INSTEAD OF TENSOR FLOW\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "from sklearn.preprocessing import Normalizer\n",
        "(train_x, train_y), (_, _) = tf.keras.datasets.boston_housing.load_data(test_split=0.2)\n",
        "train_x = train_x.astype('float32')\n",
        "\n",
        "train_y = train_y.astype('float32')\n",
        "transformer = Normalizer()\n",
        "train_x = transformer.fit_transform(train_x)\n",
        "#Now I initialized Sequential Graph using ( Model )\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "#Added Dense Layer for Prediction - KERAS DECLARES WEIGHTS AND BIAS AUTOMATICALLY\n",
        "model.add(tf.keras.layers.Dense(1, input_shape = (13,)))\n",
        "\n",
        "#NOW COMPILING THE MODEL BY ADDING LOSS AND GRADIENT DESCENT OPTIMIZER\n",
        "model.compile(optimizer='sgd', loss='mse')\n",
        "\n",
        "#Now Training the model\n",
        "model.fit(train_x,train_y,epochs=100)\n",
        "model.summary()\n",
        "model.get_weights()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBuBZGqYhqeG",
        "outputId": "a62cdc17-1e6a-4674-e585-7b214e13dffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57026/57026 [==============================] - 0s 0us/step\n",
            "Epoch 1/100\n",
            "13/13 [==============================] - 2s 3ms/step - loss: 421.7833\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 204.6855\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 126.0101\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 96.7678\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 86.2780\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 82.2154\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 80.4696\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 79.4733\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 78.8013\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 78.2488\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 77.8335\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 77.3655\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 77.0160\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 76.6319\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 76.2248\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 75.8223\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 75.4955\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 75.1782\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 74.8380\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 74.5598\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 74.2127\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 73.9162\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 73.6131\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 73.3554\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 73.0911\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 72.8384\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 72.4934\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 72.3021\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 72.0528\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 71.7920\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 71.6279\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 71.3848\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 71.1965\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 70.9486\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 70.7760\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 70.5494\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 70.3837\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 70.2526\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 70.0590\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 69.9090\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 69.7327\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 69.5720\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 69.4674\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 69.2841\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 69.1533\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 68.9975\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 68.9183\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 68.8089\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 68.5728\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 68.4593\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 68.3392\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 68.2487\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 68.1716\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 68.0698\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 67.9630\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 67.8233\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 67.7486\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 67.6883\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 67.5647\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 67.4909\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 67.3704\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 67.3190\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 67.1987\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 67.1642\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 67.1180\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 66.9910\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 66.9411\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 66.8923\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 66.7620\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 66.7597\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 66.7154\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 66.6240\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 66.5303\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 66.5279\n",
            "Epoch 75/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 66.4236\n",
            "Epoch 76/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 66.3484\n",
            "Epoch 77/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 66.2777\n",
            "Epoch 78/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 66.2867\n",
            "Epoch 79/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 66.2166\n",
            "Epoch 80/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 66.1726\n",
            "Epoch 81/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 66.1388\n",
            "Epoch 82/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 66.0430\n",
            "Epoch 83/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 66.0164\n",
            "Epoch 84/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.9385\n",
            "Epoch 85/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.9452\n",
            "Epoch 86/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.9072\n",
            "Epoch 87/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.8845\n",
            "Epoch 88/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 65.8188\n",
            "Epoch 89/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 65.7495\n",
            "Epoch 90/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.7344\n",
            "Epoch 91/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 65.6825\n",
            "Epoch 92/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.6688\n",
            "Epoch 93/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 65.6521\n",
            "Epoch 94/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.6460\n",
            "Epoch 95/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.6017\n",
            "Epoch 96/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.6094\n",
            "Epoch 97/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.5101\n",
            "Epoch 98/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 65.5133\n",
            "Epoch 99/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 65.4852\n",
            "Epoch 100/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 65.4349\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 1)                 14        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14 (56.00 Byte)\n",
            "Trainable params: 14 (56.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-1.1832516e+00],\n",
              "        [ 3.5241454e+00],\n",
              "        [-5.0271535e-01],\n",
              "        [-3.9203083e-03],\n",
              "        [-6.1001021e-01],\n",
              "        [ 1.0143864e+00],\n",
              "        [-2.0639132e-01],\n",
              "        [ 4.4487408e-01],\n",
              "        [-3.6520404e-01],\n",
              "        [-2.4805281e-01],\n",
              "        [-1.9049972e-01],\n",
              "        [ 1.6963312e+01],\n",
              "        [-1.8796339e+00]], dtype=float32),\n",
              " array([11.41692], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LABEL HOT ENCODING USING SOFTMAX\n",
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(42)\n",
        "(trainX, trainY), (testX, testY) = tf.keras.datasets.mnist.load_data()\n",
        "trainX.shape\n",
        "trainX[0] #I can see many blank ones in this one\n",
        "trainY.shape\n",
        "print(testY[0:5])\n",
        "\n",
        "#We will use Tensorflow keras and not actual keras, it is different then actual keras\n",
        "trainY = tf.keras.utils.to_categorical(trainY, num_classes=10)\n",
        "testY = tf.keras.utils.to_categorical(testY, num_classes=10)\n",
        "print(trainY.shape)\n",
        "print('The first couple of examples here are: ', trainY[0:2])\n",
        "#OUTPUT COMES AS 5 converted and 0 converted into 10 numbers\n",
        "\n",
        "#BUILDING A GRAPH\n",
        "#Now time for Model\n",
        "#SEQUENTIAL MODEL MEANS ONLY ONE INPUT AND ONLY ONE OUTPUT\n",
        "#Initializing Sequential Model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#Tried to reshape the model from 2d to 1d i.e 28x28 to 784\n",
        "model.add(tf.keras.layers.Reshape((784, ), input_shape= (28,28,))) #no input shape in this because this is sequential model\n",
        "\n",
        "#Now i worked on normalizing data where Batchnormalization is used\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "#Here I added Dense Layer which provided 10 outputs after applying softmax\n",
        "#it gives 10 wx+b and also it can create output layers\n",
        "#after getting 10y it applies softmax function so all numbers add upto 1\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax')) #using dense layer is logistic regression\n",
        "\n",
        "#Now i compiled the model\n",
        "#categorical_crossentropy for softmax, if sigmoid then binary crossentropy\n",
        "#math and formula will be different although code can run normally\n",
        "#it also calculates accuracy by passsing parameter like metrics or any other like roc, confusion, etc\n",
        "model.compile(optimizer='sgd',loss = 'categorical_crossentropy', metrics =['accuracy'])\n",
        "\n",
        "#EXECUTED THE GRAPH\n",
        "#LOSS NEEDS to be COMPARED BY USING Different Algorithms\n",
        "model.fit(trainX,trainY, validation_data=(testX,testY),epochs=10,batch_size=32)\n",
        "# to run in batches only change will be adding 32 examples that is batch size = 32, model.fit(trainX,trainY, validation_data=(testX,testY),epochs=100,batch_size=32)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#SAVED THE MODEL\n",
        "model.save('mnist_lc.h5')\n",
        "model.save('my_model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_FlOCLsQvuK",
        "outputId": "274bf973-caff-4e8d-c667-758df5dac883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7 2 1 0 4]\n",
            "(60000, 10)\n",
            "The first couple of examples here are:  [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4854 - accuracy: 0.8574 - val_loss: 0.3911 - val_accuracy: 0.9033\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3530 - accuracy: 0.8981 - val_loss: 0.3931 - val_accuracy: 0.9104\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3326 - accuracy: 0.9052 - val_loss: 0.4884 - val_accuracy: 0.9150\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3246 - accuracy: 0.9080 - val_loss: 0.4703 - val_accuracy: 0.9145\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.3172 - accuracy: 0.9099 - val_loss: 0.4351 - val_accuracy: 0.9179\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3123 - accuracy: 0.9108 - val_loss: 0.4212 - val_accuracy: 0.9180\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.3083 - accuracy: 0.9114 - val_loss: 0.3705 - val_accuracy: 0.9189\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3043 - accuracy: 0.9135 - val_loss: 0.4953 - val_accuracy: 0.9201\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3058 - accuracy: 0.9138 - val_loss: 0.4472 - val_accuracy: 0.9185\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.3018 - accuracy: 0.9146 - val_loss: 0.3880 - val_accuracy: 0.9200\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape_8 (Reshape)         (None, 784)               0         \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 784)               3136      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                7850      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10986 (42.91 KB)\n",
            "Trainable params: 9418 (36.79 KB)\n",
            "Non-trainable params: 1568 (6.12 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LABEL HOT ENCODING USING SOFTMAX\n",
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(42)\n",
        "(trainX, trainY), (testX, testY) = tf.keras.datasets.mnist.load_data()\n",
        "trainX.shape\n",
        "trainX[0] #I can see many blank ones in this one\n",
        "trainY.shape\n",
        "print(testY[0:5])\n",
        "\n",
        "#We will use Tensorflow keras and not actual keras, it is different then actual keras\n",
        "trainY = tf.keras.utils.to_categorical(trainY, num_classes=10)\n",
        "testY = tf.keras.utils.to_categorical(testY, num_classes=10)\n",
        "print(trainY.shape)\n",
        "print('The first couple of examples here are: ', trainY[0:2])\n",
        "#OUTPUT COMES AS 5 converted and 0 converted into 10 numbers\n",
        "\n",
        "#BUILDING A GRAPH\n",
        "#Now time for Model\n",
        "#SEQUENTIAL MODEL MEANS ONLY ONE INPUT AND ONLY ONE OUTPUT\n",
        "#Initializing Sequential Model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Reshape((784, ), input_shape= (28,28,))) #no input shape in this because this is sequential model\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "#Adding Hidden Layers\n",
        "#First Hideen Layer\n",
        "model.add(tf.keras.layers.Dense(200, activation='sigmoid'))\n",
        "#Second Hidden Layer\n",
        "model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
        "#Third Hidden Layer\n",
        "model.add(tf.keras.layers.Dense(60, activation='sigmoid'))\n",
        "#Fourth Hidden Layer\n",
        "model.add(tf.keras.layers.Dense(30, activation='sigmoid'))\n",
        "\n",
        "#Adding ouput Layer\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "#10 because 10 neurons\n",
        "\n",
        "#Here I added Dense Layer which provided 10 outputs after applying softmax\n",
        "#it gives 10 wx+b and also it can create output layers\n",
        "#after getting 10y it applies softmax function so all numbers add upto 1\n",
        "\n",
        "#Now i compiled the model\n",
        "#categorical_crossentropy for softmax, if sigmoid then binary crossentropy\n",
        "#math and formula will be different although code can run normally\n",
        "#it also calculates accuracy by passsing parameter like metrics or any other like roc, confusion, etc\n",
        "model.compile(optimizer='sgd',loss = 'categorical_crossentropy', metrics =['accuracy'])\n",
        "\n",
        "#EXECUTED THE GRAPH\n",
        "#LOSS NEEDS to be COMPARED BY USING Different Algorithms\n",
        "model.fit(trainX,trainY, validation_data=(testX,testY),epochs=30,batch_size=32)\n",
        "# to run in batches only change will be adding 32 examples that is batch size = 32, model.fit(trainX,trainY, validation_data=(testX,testY),epochs=100,batch_size=32)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#SAVED THE MODEL\n",
        "model.save('mnist_v2_lc.h5')\n",
        "model.save('my_model.keras')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nJbr0Wi7o8Y",
        "outputId": "dd2cc8d5-8759-4cfa-a5d2-d7046933acfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7 2 1 0 4]\n",
            "(60000, 10)\n",
            "The first couple of examples here are:  [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "Epoch 1/30\n",
            "1875/1875 [==============================] - 11s 5ms/step - loss: 2.3009 - accuracy: 0.1152 - val_loss: 2.2966 - val_accuracy: 0.1038\n",
            "Epoch 2/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 2.2944 - accuracy: 0.1262 - val_loss: 2.2904 - val_accuracy: 0.2029\n",
            "Epoch 3/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 2.2861 - accuracy: 0.1432 - val_loss: 2.2777 - val_accuracy: 0.1806\n",
            "Epoch 4/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 2.2653 - accuracy: 0.2057 - val_loss: 2.2410 - val_accuracy: 0.2964\n",
            "Epoch 5/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 2.1729 - accuracy: 0.2809 - val_loss: 2.0444 - val_accuracy: 0.2790\n",
            "Epoch 6/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 1.9067 - accuracy: 0.3272 - val_loss: 1.7841 - val_accuracy: 0.3516\n",
            "Epoch 7/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 1.7316 - accuracy: 0.4096 - val_loss: 1.6525 - val_accuracy: 0.4778\n",
            "Epoch 8/30\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 1.5869 - accuracy: 0.4743 - val_loss: 1.4669 - val_accuracy: 0.5348\n",
            "Epoch 9/30\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 1.3460 - accuracy: 0.5764 - val_loss: 1.1807 - val_accuracy: 0.6145\n",
            "Epoch 10/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 1.0960 - accuracy: 0.6495 - val_loss: 0.9689 - val_accuracy: 0.7113\n",
            "Epoch 11/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.9429 - accuracy: 0.7033 - val_loss: 0.8426 - val_accuracy: 0.7533\n",
            "Epoch 12/30\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.8483 - accuracy: 0.7403 - val_loss: 0.7617 - val_accuracy: 0.7775\n",
            "Epoch 13/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.7798 - accuracy: 0.7667 - val_loss: 0.6995 - val_accuracy: 0.8061\n",
            "Epoch 14/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.7355 - accuracy: 0.7835 - val_loss: 0.6515 - val_accuracy: 0.8238\n",
            "Epoch 15/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.6897 - accuracy: 0.7999 - val_loss: 0.6061 - val_accuracy: 0.8430\n",
            "Epoch 16/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.6426 - accuracy: 0.8187 - val_loss: 0.5634 - val_accuracy: 0.8560\n",
            "Epoch 17/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.5980 - accuracy: 0.8343 - val_loss: 0.5222 - val_accuracy: 0.8679\n",
            "Epoch 18/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.5549 - accuracy: 0.8486 - val_loss: 0.4855 - val_accuracy: 0.8787\n",
            "Epoch 19/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5159 - accuracy: 0.8587 - val_loss: 0.4523 - val_accuracy: 0.8856\n",
            "Epoch 20/30\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4876 - accuracy: 0.8662 - val_loss: 0.4239 - val_accuracy: 0.8919\n",
            "Epoch 21/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4540 - accuracy: 0.8748 - val_loss: 0.3989 - val_accuracy: 0.8959\n",
            "Epoch 22/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4330 - accuracy: 0.8808 - val_loss: 0.3788 - val_accuracy: 0.9012\n",
            "Epoch 23/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4104 - accuracy: 0.8875 - val_loss: 0.3600 - val_accuracy: 0.9070\n",
            "Epoch 24/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3841 - accuracy: 0.8942 - val_loss: 0.3453 - val_accuracy: 0.9092\n",
            "Epoch 25/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3669 - accuracy: 0.8993 - val_loss: 0.3272 - val_accuracy: 0.9161\n",
            "Epoch 26/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3510 - accuracy: 0.9020 - val_loss: 0.3146 - val_accuracy: 0.9174\n",
            "Epoch 27/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3301 - accuracy: 0.9083 - val_loss: 0.3023 - val_accuracy: 0.9200\n",
            "Epoch 28/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3155 - accuracy: 0.9136 - val_loss: 0.2898 - val_accuracy: 0.9244\n",
            "Epoch 29/30\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2988 - accuracy: 0.9184 - val_loss: 0.2736 - val_accuracy: 0.9288\n",
            "Epoch 30/30\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.2839 - accuracy: 0.9227 - val_loss: 0.2624 - val_accuracy: 0.9321\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape_3 (Reshape)         (None, 784)               0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 784)               3136      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 200)               157000    \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 100)               20100     \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 60)                6060      \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 30)                1830      \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 10)                310       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 188436 (736.08 KB)\n",
            "Trainable params: 186868 (729.95 KB)\n",
            "Non-trainable params: 1568 (6.12 KB)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "!ls -l\n",
        "\n",
        "\n",
        "model=tf.keras.models.load_model('mnist_v2_lc.h5')\n",
        "\n",
        "model.summary()\n",
        "_,(testX, testY) = tf.keras.datasets.mnist.load_data()\n",
        "model.input\n",
        "testX[0].shape #this is 2dimension model but we need 3d\n",
        "testX[0:1].shape #now I made it to 3d\n",
        "\n",
        "model.input #checking the type of input so it shows 3d\n",
        "#ALternatively using Numpy\n",
        "import numpy as np\n",
        "np.expand_dims(testX[0], axis=0).shape\n",
        "\n",
        "#actual label for first example of dataset\n",
        "print(testY[0])\n",
        "\n",
        "testX[0:1].shape\n",
        "model.input\n",
        "\n",
        "#Now I would predict the modl\n",
        "prediction = model.predict(testX[0:5]) #For 5 example using 0 to 5 or\n",
        "\n",
        "#Print Prediction\n",
        "print(prediction.shape)\n",
        "\n",
        "#Get Predicted number with highest probability\n",
        "predicted_num = np.argmax(prediction[0])\n",
        "\n",
        "#Printing the number\n",
        "print(predicted_num)\n",
        "\n",
        "testY[0]\n",
        "\n",
        "#Lets see which output will have highest probability\n",
        "predicted_num = np.argmax(prediction[0])\n",
        "\n",
        "print(predicted_num)\n",
        "\n",
        "#Now I will need to visualize it\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(testX[0],cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gv9eDBjJnJ-T",
        "outputId": "3121153a-22d1-4f3c-86e0-b8a7bbeeeb0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4532\n",
            "-rw-r--r-- 1 root root 1544824 Dec 27 00:08 mnist_lc.h5\n",
            "-rw-r--r-- 1 root root 1545096 Dec 27 00:15 mnist_v2_lc.h5\n",
            "-rw-r--r-- 1 root root 1539772 Dec 27 00:15 my_model.keras\n",
            "drwxr-xr-x 1 root root    4096 Dec 19 14:20 sample_data\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape_3 (Reshape)         (None, 784)               0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 784)               3136      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 200)               157000    \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 100)               20100     \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 60)                6060      \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 30)                1830      \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 10)                310       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 188436 (736.08 KB)\n",
            "Trainable params: 186868 (729.95 KB)\n",
            "Non-trainable params: 1568 (6.12 KB)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7c61f195cb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "(5, 10)\n",
            "7\n",
            "7\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7c61ec30f010>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}